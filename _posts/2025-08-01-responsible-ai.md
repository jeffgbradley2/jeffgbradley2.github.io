---
layout: post
title: "Responsible AI"
date: 2025-08-01
---

I'm a big believer in [Responsible AI](https://www.ibm.com/think/topics/responsible-ai). But I don't think most organizations are. In fact, I think most organizations are doing AI backward, building that "big idea" as fast as possible, putting off consequences for later. 

Responsible AI is an approach to development of AI systems that involves comprehensive [AI governance](https://www.ibm.com/think/topics/ai-governance), which encompasses the tools, rules, processes, procedures, and values these systems are developed with. It focuses on ensuring that AI systems are ethical, safe, and trustworthy. Think of it as building guardrails before stomping on the accelerator. 

But that's just a lot of words.

## What Responsible AI Actually Means 

In practice, responsible AI means adopting practices to assess the impacts of AI systems, mitigate risks, and ensure they provide value to stakeholders. This isn't an easy lift in the messy real world, where there are many competing objectives. And there isn't a one-size-fits-all approach either; what is responsible for one organization or team isn't necessarily the same for another. I've learned this firsthand.

## From Tactical to Strategic

At my current day job, we've taken this seriously. Sure, in the beginning of our AI implementation we didn't pay much heed to impact and risk, but were more interested in opportunities. But as we progressed we moved from a "tactical" approach to a "strategic" approach, and initiated formal AI Governance with [ISO 42001](https://www.iso.org/standard/42001) (the international standard for AI management systems). 

I plan on speaking more about ISO 42001 and AI Governance in the future, but it isn't very sexy. At first, it will slow things down, and add management overhead that can hinder teams at first. However, an associate of mine likes to say that "you have to go slow to go fast", and in enterprise AI this is certainly the case. The cost of mistakes is too high. One biased algorithm or hallucination in production can destroy years of customer trust in minutes.

## The Broader Social Stakes

Outside of enterprises, at the social level, the stakes are even higher. We don't really know what the impact on society will be, on children, on our ability to think for ourselves, on privacy. We're all kind of in a grand social experiment together, one we didn't sign up for.

## Our Responsibility to Act

That's why I think it is important for people like me to speak up for this and do what we can, in our own small ways, to bring about a general approach to responsible AI development, deployment, and use. We have a responsibility to act responsibly, to our families, our teams, our societies big and small, and to ourselves. 

So ask yourself: what could go wrong?