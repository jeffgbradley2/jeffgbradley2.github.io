---
layout: post
title: "MCMC MVP with PyMC"
date: 2025-08-17
description: "A practical introduction to Markov Chain Monte Carlo (MCMC) using PyMC for Bayesian A/B testing, moving beyond basic Bayes' Theorem to computational Bayesian modeling for AI evaluation."
categories: [Statistics, AI-Testing]
tags: [MCMC, PyMC, Bayesian-statistics, A/B-testing, statistical-modeling, MetaReason, probabilistic-programming, confidence-scoring]
author: Jeff Bradley
seo:
  type: BlogPosting
  canonical_url: "https://jeffgbradley2.github.io/statistics/ai-testing/2025/08/17/mcmc-mvp.html"
  keywords: "MCMC, PyMC, Bayesian statistics, A/B testing, probabilistic programming, Markov Chain Monte Carlo, statistical modeling, AI evaluation"
excerpt: "Learn practical MCMC implementation with PyMC through A/B testing examples, building computational Bayesian models for rigorous statistical analysis and AI confidence scoring."
---

Bayesian statistics makes perfect sense conceptually, but it can be tricky to implement. Our frequentist-trained intuitions often mislead us when we try to build a model, especially when choosing priors and interpreting posteriors.

I've posted about the theory before. This time, I want to build something practical. We'll tackle a common business problem using PyMC, the gold standard for probabilistic programming in Python, and build a "Minimum Viable Product" model that you can adapt for your own work.

## The Classic A/B Test

Let's start with a familiar scenario. Your boss says, "We ran an A/B test. Button A got 100 clicks from 1000 views. Button B got 115 clicks from 1000 views. Which one do we ship?"

Your gut probably tells you B is better, but you hesitate. The difference is small. How sure are you? That hesitation and the desire to express your uncertainty instead of giving a simple "yes/no" answer is the very core of Bayesian thinking. We're going to quantify that uncertainty.

## PyMC and MCMC

PyMC is a Python library for Bayesian computation. It's a "Probabilistic Programming Language" (PPL), which is a fancy way of saying it's designed to build models using probability distributions instead of single-point values.

At its heart, PyMC uses powerful algorithms like Markov Chain Monte Carlo (MCMC) to find the solution. MCMC is a method for efficiently exploring a probability distribution to find the most likely outcomes.

To understand MCMC, imagine a hiker trying to map a mountain range in a thick fog.

They can only see the ground directly beneath their feet (the data).

They take a step in a random direction. If it's uphill (a more likely value), they'll probably take that step. If it's downhill (less likely), they might still go there occasionally to avoid getting stuck on a small hill.

After thousands of steps, the path they've walked creates a detailed map of the entire mountain range. This map is our posterior distribution. It doesn't just show the single highest peak, but all the plausible values.

In this analogy, the hiker's path is the "Markov Chain," a series of steps where the next position depends only on the current one. It's a clever way to map a complex space without having to visit every single point.

## PyMC MVP

So, how do we model our A/B test? This is where we have to start thinking in distributions.

We're modeling a number of successes (clicks) out of a fixed number of trials (views). This is a classic Binomial distribution scenario. In Bayesian statistics, a common choice for the prior on a Binomial probability is the Beta distribution. This pairing is known as a "conjugate prior," which has nice mathematical properties, but more importantly, it's flexible and intuitive. The Beta distribution is perfect for representing the uncertainty about a probability between 0 and 1.

With our distributions chosen, we can follow the standard modeling workflow:

1. Define the model structure.
2. Condition the model on observed data.
3. Sample from the posterior distribution to evaluate the results.

Let's do it in code.

## PyMC Walkthrough

Here is the complete model, from data setup to visualization.

```python
import pymc as pm
import numpy as np
import arviz as az
import matplotlib.pyplot as plt

# 1. Data Setup
# (clicks, views) for A and B
y_obs = np.array([100, 115]) # successes
n_views = np.array([1000, 1000]) # trials

# 2. Model Definition
with pm.Model() as model_ab:
    # Priors: An independent conversion rate parameter for each button (A and B).
    # We use a Beta(1,1) prior, which is uniform from 0 to 1,
    # representing that we have no initial preference for any conversion rate.
    theta = pm.Beta('theta', alpha=1.0, beta=1.0, shape=2)

    # Likelihood: The Binomial distribution connects our data to the priors.
    y = pm.Binomial('y', n=n_views, p=theta, observed=y_obs)

    # 3. Deterministic Variables for Analysis
    # These are calculated from the posterior samples of theta.
    # They directly answer our business questions.
    diff = pm.Deterministic('difference', theta[1] - theta[0])
    uplift = pm.Deterministic('uplift', (theta[1] - theta[0]) / theta[0])

    # 4. Sampling from the Posterior
    idata = pm.sample()

# 5. Analysis and Visualization
theta_a_samples = idata.posterior['theta'].sel(theta_dim_0=0).values.flatten()
theta_b_samples = idata.posterior['theta'].sel(theta_dim_0=1).values.flatten()

# Create a new figure with two rows
fig, axes = plt.subplots(2, 1, figsize=(8, 6), constrained_layout=True)

# Plot the posterior densities for each button's conversion rate
az.plot_kde(theta_a_samples, ax=axes[0], label="Button A", fill_kwargs={"alpha": 0.1})
az.plot_kde(theta_b_samples, ax=axes[0], label="Button B", fill_kwargs={"alpha": 0.1})
axes[0].set_title("Posterior Distributions of Conversion Rates", fontsize=14)
axes[0].legend()

# Plot the posterior of the difference between B and A
prob_b_better = (idata.posterior['difference'] > 0).mean().item()
az.plot_posterior(idata, var_names=['difference'], ax=axes[1])
axes[1].set_title(f"B is better than A with {prob_b_better:.1%} probability", fontsize=14)

fig.suptitle("A/B Test Analysis", fontsize=16, y=1.03)
plt.show()
```

![A/B Test Analysis](/assets/images/2025-08-17-ab-test.png)

The plots give us a rich, actionable answer. The top plot shows the full range of plausible conversion rates for both buttons. We can see that Button B's distribution is shifted to the right, but there's significant overlap.

The bottom plot makes the conclusion clear: there is an 85.9% probability that Button B is better than Button A. We can also see that the most likely improvement is around 1.5 percentage points, but the true difference could plausibly range from -1.1 to +4.2 percentage points. Now you can go back to your boss with a nuanced, data-driven recommendation.

## From Guesswork to Confidence

This is the power of PyMC. It abstracts away the complex math and lets us focus on the model and its results. We've moved from a simple gut feeling to a statistically rigorous estimate that explicitly states our confidence.

This ability to quantify uncertainty is critical for risk management, accurate forecasting, and making better business decisions. We've conditioned this model on our data, and we could even use it to predict the outcome of future tests (using the posterior predictive distribution), but that's a topic for another day.