---
layout: post
date: 2026-01-08
title: "Human in the Loop"
tags: [engineering, ai]
---

Over the past few months, I've been building a new AI system for our sister company. They came to us with several processes they wanted to explore automating and augmenting. Like many operations and executive teams, they just wanted to see what AI could do for them, and if it could help them scale.

All AI projects that we undertake fall within the domain of our AIMS, so I leaned on the phases of our AI SDLC. I traveled with my CTO and the project manager to their site, where we spent a few days on the Problem Definition phase of our lifecycle. We developed detailed Use Case Specifications, and from those I developed the formal Problem Definitions required by our governance framework.

I continued through the phases for each of the candidate processes we identified as highest value, performing Impact and Risk Assessments. It was at this stage, as I assessed risk, that I realized just how risky what the executives were asking was. We assessed multiple high risk elements that would have to be addressed if we were to move forward. But risk wasn't at the top of their minds. They were interested in automation.

Like many leadership teams right now, they were eager to throw AI at the problem. They were attending conferences where companies were giving fancy demos of AI systems that could basically do _everything_: call carriers, handle customer support, negotiate contracts, and more. And they were hearing from other industry leaders on their blogs and LinkedIn feeds about the amazing success they were having with _agents_, automating all the things.

As I moved into the system design phase, I began meeting with operators, interviewing them, and documenting their processes. I found that the interface between operators and their customers was one of our highest-risk points in the process: there was potential to provide incorrect information, damage relationships, or harm reputation. But it was also the most _human_ step. Operators were bringing an enormous amount of context to bear, tribal knowledge, and their relationships with clients who _valued_ that service.

So, the highest risk portions of the process that we had targeted to automate first turned out to be the most nuanced _and_ the most valuable for the clients. The parts that weren't as risky or nuanced were the more mechanical steps: searching for the shipment in one system, looking up some information in another, going to a carrier website and searching there, sending an email to the carrier, and a follow up when they don't respond, then drafting a response to their client.

I saw an opportunity. I presented a case, based on industry reports and best practices for what _not_ to do (just automate everything right out of the gate), and proposed my solution: a Logistics Copilot that would work _with and for_ the operators. It wouldn't replace them, but support them, handling the repetitive, low-value, low-skill tasks while leaving the nuanced and highly contextual tasks to the humans.

But first, I had to get buy-in from my leadership. I was following the process, not pitching a pet project, and it led me here. So I created a detailed proposal for the Logistics Copilot, the reasoning and risks it would mitigate, while allowing us to learn and evolve. The data that was generated by the copilot, the lessons we learned, would all be used to continually improve and fully vet the system before just flipping the switch to full autopilot.

My leadership loved the idea. And so, at the next intercompany meeting, I presented it to all stakeholders across the organizations. I shared the artifacts of our process, the impact and risk assessments, the findings from our discovery meetings, and the copilot solution. The idea was met with appreciation and _relief_, as multiple stakeholders and even one of the CEOs said how nervous they were about simply automating the process. The solution specifically addressed their fears, kept operators in the driver's seat, automated the mind-numbing repetitive tasks, and left the door open to increased automation in the future if and when the data showed we are ready.

That was some time ago. In the months that have followed, I've built an enterprise-grade AI system that autonomously works "tickets" for operators. It queries source system APIs, pulls data from carrier websites when APIs aren't available or data is stale, sends emails and follow-ups, and drafts responses for human review. The trickiest part turned out to be meeting the operators where they already work: in Outlook. I pushed for a plugin despite initial reservations from my leadership (which I understood completely), and it turned out to be the right call. Adoption depends on friction, and adding another app for the operators to log in to would have killed it.

I'm writing now because we just deployed the Alpha version. There is still a lot of testing and tweaking to do, and there are more controls to put in place before the Beta goes to the real users. Plenty of unknowns, lots to discover. But it has been a success to this point, one that may lead to a whole new product for our company.

---

It pays to be systematic in assessing the impacts and risks of a system, to do detailed discovery and requirements gathering, and to listen closely to the people who are doing the work. And it's important to strive to find the best solution, even if it isn't what the client has in mind, and even if it isn't a simple win. We have to be willing to change course, to stand up and say "we're at risk if we continue down this path" and to provide an alternative. That's the job.
